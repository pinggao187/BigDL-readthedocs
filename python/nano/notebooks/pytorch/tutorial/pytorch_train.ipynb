{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook we will demonstrates how to use BigDL-Nano to accelerate PyTorch or PyTorch-Lightning applications on training workloads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Environment\n",
    "Before you start with Apis delivered by bigdl-nano, you have to make sure BigDL-Nano is correctly installed for PyTorch. If not, please follow [this](../../../../../docs/readthedocs/source/doc/Nano/Overview/nano.md) to set up your environment.<br>\n",
    "\n",
    "We used pre-built cifar10 datamodule from lightning-bolts for demo. You are required to install lightnig-bolts as follows:\n",
    "```python\n",
    "pip install lightning-bolts\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Cifar10 DataModule\n",
    "Import the existing data module from bolts and modify the train and test transforms.\n",
    "You could access [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html) for a view of the whole dataset.\n",
    "Leveraging OpenCV and libjpeg-turbo, BigDL-Nano can accelerate computer vision data pipelines by providing a drop-in replacement of torch_vision's `datasets` and `transforms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pl_bolts.datamodules import CIFAR10DataModule\n",
    "from pl_bolts.transforms.dataset_normalizations import cifar10_normalization\n",
    "from bigdl.nano.pytorch.vision import transforms\n",
    "DATA_PATH = os.environ.get('DATA_PATH', '.')\n",
    "BATCH_SIZE = 64\n",
    "DEV_RUN = bool(os.environ.get('DEV_RUN', False))\n",
    "train_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, 4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        cifar10_normalization()\n",
    "    ]\n",
    ")\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        cifar10_normalization()\n",
    "    ]\n",
    ")\n",
    "cifar10_dm = CIFAR10DataModule(\n",
    "    data_dir = DATA_PATH,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    train_transforms = train_transforms,\n",
    "    val_transforms = test_transforms,\n",
    "    test_transforms = test_transforms\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Custom Model\n",
    "Modify the pre-existing Resnet architecture from TorchVision. The pre-existing architecture is based on ImageNet images (224x224) as input. So we need to modify it for CIFAR10 images (32x32)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from torchvision.models import resnet18\n",
    "from pytorch_lightning import LightningModule, seed_everything\n",
    "from torchmetrics.functional import accuracy\n",
    "seed_everything(7)\n",
    "def create_model():\n",
    "    model = resnet18(pretrained=False, num_classes=10)\n",
    "    model.conv1 = nn.Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
    "    model.maxpool = nn.Identity()\n",
    "    return model\n",
    "\n",
    "class LitResnet(LightningModule):\n",
    "    def __init__(self, learning_rate=0.05, num_processes=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = create_model()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "        return F.log_softmax(out, dim=1)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def evaluate(self, batch, stage=None):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = F.nll_loss(logits, y)\n",
    "        preds = torch.argmax(logits, dim=1)\n",
    "        acc = accuracy(preds, y)\n",
    "\n",
    "        if stage:\n",
    "            self.log(f\"{stage}_loss\", loss, prog_bar=True)\n",
    "            self.log(f\"{stage}_acc\", acc, prog_bar=True)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"val\")\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        self.evaluate(batch, \"test\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(\n",
    "            self.parameters(),\n",
    "            lr=self.hparams.learning_rate,\n",
    "            momentum=0.9,\n",
    "            weight_decay=5e-4,\n",
    "        )\n",
    "        steps_per_epoch = 45000 // BATCH_SIZE // self.hparams.num_processes\n",
    "        scheduler_dict = {\n",
    "            \"scheduler\": OneCycleLR(\n",
    "                optimizer,\n",
    "                0.1,\n",
    "                epochs=self.trainer.max_epochs,\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "            ),\n",
    "            \"interval\": \"step\",\n",
    "        }\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler_dict}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with Nano Apis\n",
    "The PyTorch Trainer (`bigdl.nano.pytorch.Trainer`) is the place where we integrate most optimizations. It extends PyTorch Lightning's Trainer and has a few more parameters and methods specific to BigDL-Nano. The Trainer can be directly used to train a `LightningModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nano.pytorch import Trainer\n",
    "model = LitResnet()\n",
    "model.datamodule = cifar10_dm\n",
    "trainer = Trainer(max_epochs=30,\n",
    "                  fast_dev_run=DEV_RUN) # run model once quickly in test\n",
    "fit_time_basic = %timeit -n 1 -r 1 -o \\\n",
    "trainer.fit(model, datamodule=cifar10_dm)\n",
    "metric_basic = trainer.test(model, datamodule=cifar10_dm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intel Extension for Pytorch (a.k.a. IPEX) link extends PyTorch with optimizations for an extra performance boost on Intel hardware. BigDL-Nano integrates IPEX through the Trainer. Users can turn on IPEX by setting use_ipex=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitResnet()\n",
    "model.datamodule = cifar10_dm\n",
    "trainer = Trainer(max_epochs=30, \n",
    "                  use_ipex=True,\n",
    "                  fast_dev_run=DEV_RUN)\n",
    "fit_time_ipex = %timeit -n 1 -r 1 -o \\\n",
    "trainer.fit(model, datamodule=cifar10_dm)\n",
    "metric_ipex = trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting use_ipex=True will Apply optimizations at Python frontend to the given model (nn.Module), as well as the given optimizer (optional). Optimizations include conv+bn folding (for inference only), weight prepacking and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the number of processes on distributed training to accelerate training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitResnet(learning_rate=0.1, num_processes=4)\n",
    "model.datamodule = cifar10_dm\n",
    "trainer = Trainer(max_epochs=30, \n",
    "                  num_processes=4,\n",
    "                  fast_dev_run=DEV_RUN)\n",
    "fit_time_dit = %timeit -n 1 -r 1 -o \\\n",
    "trainer.fit(model, datamodule=cifar10_dm)\n",
    "metric_dit = trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable both distributed training and ipex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitResnet(learning_rate=0.1, num_processes=4)\n",
    "model.datamodule = cifar10_dm\n",
    "trainer = Trainer(max_epochs=30, \n",
    "                  num_processes=4,\n",
    "                  use_ipex=True,\n",
    "                  fast_dev_run=DEV_RUN)\n",
    "fit_time_dit_ipex = %timeit -n 1 -r 1 -o \\\n",
    "trainer.fit(model, datamodule=cifar10_dm)\n",
    "metric_dit_ipex = trainer.test(model, datamodule=cifar10_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.10 ('testNotebook')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f74e20bd6be619d8bad0cfdf7f83080da032bdcafc0d029c8a063b1b072aff02"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
